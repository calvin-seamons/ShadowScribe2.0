{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation: Joint DeBERTa-v3 Model vs Claude Haiku 4.5 Baseline\n",
    "\n",
    "This notebook evaluates our fine-tuned joint DeBERTa-v3 model against Claude Haiku 4.5 (the current LLM-based approach in our RAG system).\n",
    "\n",
    "## Models Compared:\n",
    "1. **Claude Haiku 4.5 (Baseline)** - Current LLM-based tool/intent selection via API\n",
    "2. **Our Joint DeBERTa-v3 Model** - Fine-tuned transformer for all three tasks\n",
    "\n",
    "## Metrics:\n",
    "- **Tool Selection**: F1, Precision, Recall, Exact Match\n",
    "- **Intent Classification**: F1, Precision, Recall, Exact Match\n",
    "- **NER**: Entity-level F1, Precision, Recall (DeBERTa only)\n",
    "- **Latency**: Average inference time per query\n",
    "- **Cost**: API cost vs local inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers torch scikit-learn seqeval matplotlib pandas\n",
    "!pip install -q pytorch-crf anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Mount Google Drive\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Setup paths for Google Colab\nimport os\nfrom pathlib import Path\n\nPROJECT_ROOT = Path('/content/drive/MyDrive/574-assignment')\nDATA_PATH = PROJECT_ROOT / 'data' / 'generated'\nMODEL_PATH = PROJECT_ROOT / 'models' / 'joint_deberta'\n\n# Create results directory\nRESULTS_PATH = PROJECT_ROOT / 'results'\nRESULTS_PATH.mkdir(parents=True, exist_ok=True)\n\n# Get API key from Colab secrets or environment\nfrom google.colab import userdata\ntry:\n    ANTHROPIC_API_KEY = userdata.get('ANTHROPIC_API_KEY')\nexcept Exception:\n    ANTHROPIC_API_KEY = os.environ.get('ANTHROPIC_API_KEY', '')\n\nprint(f\"Project root: {PROJECT_ROOT}\")\nprint(f\"Data path: {DATA_PATH}\")\nprint(f\"Model path: {MODEL_PATH}\")\nprint(f\"Results path: {RESULTS_PATH}\")\nprint(f\"API key configured: {'Yes' if ANTHROPIC_API_KEY else 'No - add ANTHROPIC_API_KEY to Colab Secrets'}\")\n\n# Verify data files exist\nprint(\"\\nChecking data files:\")\nfor f in ['train.json', 'val.json', 'test.json', 'label_mappings.json']:\n    if (DATA_PATH / f).exists():\n        print(f\"  ✓ Found {f}\")\n    else:\n        print(f\"  ✗ Missing {f}\")\n\n# Check if trained model exists\nprint(\"\\nChecking trained model:\")\nif (MODEL_PATH / 'config.json').exists():\n    print(\"  ✓ Found trained model\")\nelse:\n    print(\"  ✗ Model not found - run notebook 02 first to train the model\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import asyncio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import (\n",
    "    f1_score, precision_score, recall_score, \n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "from seqeval.metrics import f1_score as seqeval_f1, classification_report as seqeval_report\n",
    "from collections import Counter, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import anthropic\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Label Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load data\ndef load_data(split):\n    with open(DATA_PATH / f'{split}.json', 'r') as f:\n        return json.load(f)\n\ntrain_data = load_data('train')\nval_data = load_data('val')\ntest_data = load_data('test')\n\nprint(f\"Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load label mappings from trained model\nwith open(MODEL_PATH / 'label_mappings.json', 'r') as f:\n    mappings = json.load(f)\n\nTOOL_TO_IDX = mappings['tool_to_idx']\nINTENT_TO_IDX = mappings['intent_to_idx']\nTAG_TO_IDX = mappings['tag_to_idx']\nINTENT_TO_TOOL = mappings['intent_to_tool']\n\nIDX_TO_TOOL = {int(v): k for k, v in TOOL_TO_IDX.items()}\nIDX_TO_INTENT = {int(v): k for k, v in INTENT_TO_IDX.items()}\nIDX_TO_TAG = {int(v): k for k, v in TAG_TO_IDX.items()}\n\nTOOLS = list(TOOL_TO_IDX.keys())\nALL_INTENTS = list(INTENT_TO_IDX.keys())\nNER_TAGS = list(TAG_TO_IDX.keys())\n\n# Group intents by tool for prompt\nTOOL_INTENTS = defaultdict(list)\nfor intent, tool in INTENT_TO_TOOL.items():\n    TOOL_INTENTS[tool].append(intent)\n\nprint(f\"Tools: {TOOLS}\")\nprint(f\"Total intents: {len(ALL_INTENTS)}\")\nprint(f\"NER tags: {len(NER_TAGS)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data distribution analysis\n",
    "def analyze_distribution(data, name):\n",
    "    tool_counts = Counter()\n",
    "    intent_counts = Counter()\n",
    "    num_tools = Counter()\n",
    "    \n",
    "    for sample in data:\n",
    "        for tool in sample['tools']:\n",
    "            tool_counts[tool] += 1\n",
    "        for intent in sample['intents'].values():\n",
    "            intent_counts[intent] += 1\n",
    "        num_tools[len(sample['tools'])] += 1\n",
    "    \n",
    "    print(f\"\\n{name} Distribution:\")\n",
    "    print(f\"  Tool counts: {dict(tool_counts)}\")\n",
    "    print(f\"  Num tools per query: {dict(num_tools)}\")\n",
    "    print(f\"  Top 5 intents: {intent_counts.most_common(5)}\")\n",
    "    \n",
    "    return tool_counts, intent_counts, num_tools\n",
    "\n",
    "train_dist = analyze_distribution(train_data, \"Train\")\n",
    "test_dist = analyze_distribution(test_data, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Claude Haiku 4.5 Baseline\n",
    "\n",
    "This replicates the LLM-based tool/intent selection currently used in our RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClaudeHaikuBaseline:\n",
    "    \"\"\"\n",
    "    Claude Haiku 4.5 baseline for tool and intent selection.\n",
    "    Mirrors the current LLM-based approach in ShadowScribe's central_engine.py\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, api_key, model=\"claude-haiku-4-5-20251001\"):\n",
    "        self.client = anthropic.Anthropic(api_key=api_key)\n",
    "        self.model = model\n",
    "        self.total_input_tokens = 0\n",
    "        self.total_output_tokens = 0\n",
    "        self.total_latency = 0\n",
    "        self.num_calls = 0\n",
    "    \n",
    "    def _get_tool_selector_prompt(self, query):\n",
    "        \"\"\"Generate the tool and intent selection prompt (mirrors central_engine.py)\"\"\"\n",
    "        return f\"\"\"You are a query router for a D&D 5e RAG system. Analyze the user's query and determine:\n",
    "1. Which tools are needed to answer the query\n",
    "2. The specific intention for each tool\n",
    "\n",
    "Available tools and their intents:\n",
    "\n",
    "**character_data** - For queries about the player's character sheet\n",
    "Intents: {', '.join(TOOL_INTENTS['character_data'])}\n",
    "\n",
    "**session_notes** - For queries about campaign history, past sessions, NPCs encountered\n",
    "Intents: {', '.join(TOOL_INTENTS['session_notes'])}\n",
    "\n",
    "**rulebook** - For D&D 5e rules, spells, classes, mechanics\n",
    "Intents: {', '.join(TOOL_INTENTS['rulebook'])}\n",
    "\n",
    "User Query: \"{query}\"\n",
    "\n",
    "Respond with ONLY valid JSON in this exact format:\n",
    "{{\n",
    "  \"tools_needed\": [\n",
    "    {{{{\n",
    "      \"tool\": \"<tool_name>\",\n",
    "      \"intention\": \"<intent_name>\",\n",
    "      \"confidence\": <0.0-1.0>\n",
    "    }}}}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Rules:\n",
    "- Select 1-3 tools based on query needs\n",
    "- Each tool can only appear once\n",
    "- Choose the most specific matching intention for each tool\n",
    "- Multi-part queries may need multiple tools\"\"\"\n",
    "    \n",
    "    def _get_entity_extraction_prompt(self, query):\n",
    "        \"\"\"Generate the entity extraction prompt (mirrors central_engine.py)\"\"\"\n",
    "        return f\"\"\"Extract all D&D-related named entities from this query.\n",
    "\n",
    "Entity types to look for:\n",
    "- SPELL: Spell names (e.g., \"Fireball\", \"Cure Wounds\")\n",
    "- CLASS: Character classes (e.g., \"Wizard\", \"Fighter\")\n",
    "- RACE: Character races (e.g., \"Elf\", \"Dwarf\")\n",
    "- CREATURE: Monster/creature names (e.g., \"Dragon\", \"Goblin\")\n",
    "- ITEM: Equipment/magic items (e.g., \"Longsword\", \"Bag of Holding\")\n",
    "- LOCATION: Place names (e.g., \"Waterdeep\", \"Undermountain\")\n",
    "- ABILITY: Ability scores (e.g., \"Strength\", \"Dexterity\")\n",
    "- SKILL: Skills (e.g., \"Perception\", \"Stealth\")\n",
    "- CONDITION: Conditions (e.g., \"Poisoned\", \"Stunned\")\n",
    "- DAMAGE_TYPE: Damage types (e.g., \"fire\", \"radiant\")\n",
    "- FEAT: Feats (e.g., \"Great Weapon Master\")\n",
    "- BACKGROUND: Backgrounds (e.g., \"Noble\", \"Outlander\")\n",
    "\n",
    "Query: \"{query}\"\n",
    "\n",
    "Respond with ONLY valid JSON:\n",
    "{{\n",
    "  \"entities\": [\n",
    "    {{\"name\": \"<entity_text>\", \"type\": \"<ENTITY_TYPE>\", \"confidence\": <0.0-1.0>}}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "If no entities found, return {{\"entities\": []}}\"\"\"\n",
    "    \n",
    "    def _parse_json_response(self, text, default):\n",
    "        \"\"\"Parse JSON from LLM response, handling common issues.\"\"\"\n",
    "        try:\n",
    "            # Try direct parse\n",
    "            return json.loads(text)\n",
    "        except json.JSONDecodeError:\n",
    "            # Try to extract JSON from markdown code blocks\n",
    "            import re\n",
    "            json_match = re.search(r'```(?:json)?\\s*([\\s\\S]*?)```', text)\n",
    "            if json_match:\n",
    "                try:\n",
    "                    return json.loads(json_match.group(1))\n",
    "                except:\n",
    "                    pass\n",
    "            # Try to find JSON object in text\n",
    "            json_match = re.search(r'\\{[\\s\\S]*\\}', text)\n",
    "            if json_match:\n",
    "                try:\n",
    "                    return json.loads(json_match.group(0))\n",
    "                except:\n",
    "                    pass\n",
    "            return default\n",
    "    \n",
    "    def predict_single(self, query):\n",
    "        \"\"\"Make predictions for a single query using Claude Haiku 4.5.\"\"\"\n",
    "        # Tool and intent selection\n",
    "        start_time = time.time()\n",
    "        \n",
    "        tool_response = self.client.messages.create(\n",
    "            model=self.model,\n",
    "            max_tokens=500,\n",
    "            temperature=0,\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": self._get_tool_selector_prompt(query)\n",
    "            }]\n",
    "        )\n",
    "        \n",
    "        tool_latency = time.time() - start_time\n",
    "        self.total_input_tokens += tool_response.usage.input_tokens\n",
    "        self.total_output_tokens += tool_response.usage.output_tokens\n",
    "        \n",
    "        # Entity extraction\n",
    "        start_time = time.time()\n",
    "        \n",
    "        entity_response = self.client.messages.create(\n",
    "            model=self.model,\n",
    "            max_tokens=500,\n",
    "            temperature=0,\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": self._get_entity_extraction_prompt(query)\n",
    "            }]\n",
    "        )\n",
    "        \n",
    "        entity_latency = time.time() - start_time\n",
    "        self.total_input_tokens += entity_response.usage.input_tokens\n",
    "        self.total_output_tokens += entity_response.usage.output_tokens\n",
    "        \n",
    "        self.total_latency += tool_latency + entity_latency\n",
    "        self.num_calls += 2\n",
    "        \n",
    "        # Parse responses\n",
    "        tool_text = tool_response.content[0].text\n",
    "        entity_text = entity_response.content[0].text\n",
    "        \n",
    "        tool_data = self._parse_json_response(tool_text, {\"tools_needed\": []})\n",
    "        entity_data = self._parse_json_response(entity_text, {\"entities\": []})\n",
    "        \n",
    "        # Extract tools and intents\n",
    "        tools = []\n",
    "        intents = {}\n",
    "        \n",
    "        for tool_info in tool_data.get(\"tools_needed\", []):\n",
    "            tool_name = tool_info.get(\"tool\", \"\")\n",
    "            intent_name = tool_info.get(\"intention\", \"\")\n",
    "            \n",
    "            if tool_name in TOOLS:\n",
    "                tools.append(tool_name)\n",
    "                # Validate intent belongs to tool\n",
    "                if intent_name in TOOL_INTENTS.get(tool_name, []):\n",
    "                    intents[tool_name] = intent_name\n",
    "                else:\n",
    "                    # Fallback to first intent for tool\n",
    "                    intents[tool_name] = TOOL_INTENTS[tool_name][0]\n",
    "        \n",
    "        # Fallback if no tools predicted\n",
    "        if not tools:\n",
    "            tools = ['rulebook']\n",
    "            intents = {'rulebook': 'spell_details'}\n",
    "        \n",
    "        # Extract entities with BIO tags\n",
    "        entities = entity_data.get(\"entities\", [])\n",
    "        \n",
    "        return tools, intents, entities\n",
    "    \n",
    "    def predict_batch(self, queries, batch_size=10, delay=0.5):\n",
    "        \"\"\"Predict for multiple queries with rate limiting.\"\"\"\n",
    "        all_tools = []\n",
    "        all_intents = []\n",
    "        all_entities = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(queries), batch_size), desc=\"Claude Haiku 4.5 inference\"):\n",
    "            batch = queries[i:i+batch_size]\n",
    "            \n",
    "            for query in batch:\n",
    "                try:\n",
    "                    tools, intents, entities = self.predict_single(query)\n",
    "                    all_tools.append(tools)\n",
    "                    all_intents.append(intents)\n",
    "                    all_entities.append(entities)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error on query: {query[:50]}... - {e}\")\n",
    "                    all_tools.append(['rulebook'])\n",
    "                    all_intents.append({'rulebook': 'spell_details'})\n",
    "                    all_entities.append([])\n",
    "                \n",
    "                time.sleep(delay)  # Rate limiting\n",
    "        \n",
    "        return all_tools, all_intents, all_entities\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get usage statistics.\"\"\"\n",
    "        # Pricing: $1/$5 per million input/output tokens\n",
    "        input_cost = (self.total_input_tokens / 1_000_000) * 1.0\n",
    "        output_cost = (self.total_output_tokens / 1_000_000) * 5.0\n",
    "        total_cost = input_cost + output_cost\n",
    "        avg_latency = self.total_latency / max(self.num_calls, 1) * 1000  # ms\n",
    "        \n",
    "        return {\n",
    "            'total_input_tokens': self.total_input_tokens,\n",
    "            'total_output_tokens': self.total_output_tokens,\n",
    "            'total_cost_usd': total_cost,\n",
    "            'avg_latency_ms': avg_latency,\n",
    "            'num_api_calls': self.num_calls\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Claude Haiku 4.5 baseline\n",
    "haiku_baseline = ClaudeHaikuBaseline(api_key=ANTHROPIC_API_KEY)\n",
    "print(f\"Initialized Claude Haiku 4.5 baseline with model: {haiku_baseline.model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Our DeBERTa Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from transformers import DebertaV2Tokenizer, DebertaV2Model, DebertaV2PreTrainedModel, AutoConfig\nfrom torchcrf import CRF\n\nclass JointDeBERTaModel(DebertaV2PreTrainedModel):\n    \"\"\"Joint model for tool/intent classification and NER.\"\"\"\n    \n    def __init__(self, config, num_tools=3, num_intents=61, num_ner_tags=25):\n        super().__init__(config)\n        self.num_tools = num_tools\n        self.num_intents = num_intents\n        self.num_ner_tags = num_ner_tags\n        \n        self.deberta = DebertaV2Model(config)\n        classifier_dropout = getattr(config, 'classifier_dropout', config.hidden_dropout_prob)\n        self.dropout = nn.Dropout(classifier_dropout)\n        \n        self.tool_classifier = nn.Sequential(\n            nn.Linear(config.hidden_size, config.hidden_size),\n            nn.GELU(),\n            nn.Dropout(classifier_dropout),\n            nn.Linear(config.hidden_size, num_tools)\n        )\n        \n        self.intent_classifier = nn.Sequential(\n            nn.Linear(config.hidden_size, config.hidden_size),\n            nn.GELU(),\n            nn.Dropout(classifier_dropout),\n            nn.Linear(config.hidden_size, num_intents)\n        )\n        \n        self.ner_classifier = nn.Sequential(\n            nn.Linear(config.hidden_size, config.hidden_size // 2),\n            nn.GELU(),\n            nn.Dropout(classifier_dropout),\n            nn.Linear(config.hidden_size // 2, num_ner_tags)\n        )\n        self.crf = CRF(num_ner_tags, batch_first=True)\n        self.post_init()\n    \n    def forward(self, input_ids=None, attention_mask=None, **kwargs):\n        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n        sequence_output = outputs.last_hidden_state\n        cls_output = self.dropout(sequence_output[:, 0, :])\n        \n        tool_logits = self.tool_classifier(cls_output)\n        intent_logits = self.intent_classifier(cls_output)\n        ner_emissions = self.ner_classifier(sequence_output)\n        \n        return {\n            'tool_logits': tool_logits,\n            'intent_logits': intent_logits,\n            'ner_emissions': ner_emissions\n        }\n    \n    def decode_ner(self, ner_emissions, attention_mask):\n        return self.crf.decode(ner_emissions, mask=attention_mask.bool())\n\n# Load model\nprint(\"Loading trained DeBERTa model...\")\nwith open(MODEL_PATH / 'training_config.json', 'r') as f:\n    CONFIG = json.load(f)\n\ntokenizer = DebertaV2Tokenizer.from_pretrained(MODEL_PATH)\nmodel = JointDeBERTaModel.from_pretrained(\n    MODEL_PATH,\n    num_tools=len(TOOLS),\n    num_intents=len(ALL_INTENTS),\n    num_ner_tags=len(NER_TAGS)\n)\nmodel = model.to(device)\nmodel.eval()\nprint(\"Model loaded.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deberta_predict(model, tokenizer, queries, device, batch_size=32):\n",
    "    \"\"\"Run DeBERTa model prediction on queries with timing.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    tool_preds = []\n",
    "    intent_preds = []\n",
    "    all_ner_preds = []\n",
    "    total_time = 0\n",
    "    \n",
    "    for i in tqdm(range(0, len(queries), batch_size), desc=\"DeBERTa inference\"):\n",
    "        batch_queries = queries[i:i+batch_size]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = tokenizer(\n",
    "            batch_queries,\n",
    "            max_length=CONFIG['max_length'],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].to(device)\n",
    "        attention_mask = encoding['attention_mask'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        batch_time = time.time() - start_time\n",
    "        total_time += batch_time\n",
    "        \n",
    "        # Tool predictions\n",
    "        tool_probs = torch.sigmoid(outputs['tool_logits']).cpu().numpy()\n",
    "        for probs in tool_probs:\n",
    "            pred_tools = [TOOLS[j] for j, p in enumerate(probs) if p > 0.5]\n",
    "            if not pred_tools:\n",
    "                pred_tools = [TOOLS[np.argmax(probs)]]\n",
    "            tool_preds.append(pred_tools)\n",
    "        \n",
    "        # Intent predictions\n",
    "        intent_probs = torch.sigmoid(outputs['intent_logits']).cpu().numpy()\n",
    "        for j, (probs, tools) in enumerate(zip(intent_probs, tool_preds[-len(batch_queries):])):\n",
    "            pred_intents = {}\n",
    "            for tool in tools:\n",
    "                tool_intents = [intent for intent, t in INTENT_TO_TOOL.items() if t == tool]\n",
    "                tool_intent_probs = [(intent, probs[INTENT_TO_IDX[intent]]) for intent in tool_intents]\n",
    "                best_intent = max(tool_intent_probs, key=lambda x: x[1])\n",
    "                pred_intents[tool] = best_intent[0]\n",
    "            intent_preds.append(pred_intents)\n",
    "        \n",
    "        # NER predictions\n",
    "        ner_preds_batch = model.decode_ner(outputs['ner_emissions'], attention_mask)\n",
    "        for pred_seq in ner_preds_batch:\n",
    "            pred_tags = [IDX_TO_TAG[idx] for idx in pred_seq]\n",
    "            all_ner_preds.append(pred_tags)\n",
    "    \n",
    "    avg_latency = (total_time / len(queries)) * 1000  # ms per query\n",
    "    \n",
    "    return tool_preds, intent_preds, all_ner_preds, avg_latency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(true_data, tool_preds, intent_preds, ner_preds=None):\n",
    "    \"\"\"Compute all evaluation metrics.\"\"\"\n",
    "    \n",
    "    # Tool metrics\n",
    "    true_tools = [set(s['tools']) for s in true_data]\n",
    "    pred_tools = [set(t) for t in tool_preds]\n",
    "    \n",
    "    mlb = MultiLabelBinarizer(classes=TOOLS)\n",
    "    y_true_tools = mlb.fit_transform([s['tools'] for s in true_data])\n",
    "    y_pred_tools = mlb.transform(tool_preds)\n",
    "    \n",
    "    tool_f1 = f1_score(y_true_tools, y_pred_tools, average='micro')\n",
    "    tool_precision = precision_score(y_true_tools, y_pred_tools, average='micro')\n",
    "    tool_recall = recall_score(y_true_tools, y_pred_tools, average='micro')\n",
    "    tool_exact_match = np.mean([t == p for t, p in zip(true_tools, pred_tools)])\n",
    "    \n",
    "    # Intent metrics\n",
    "    true_intents = [set(s['intents'].values()) for s in true_data]\n",
    "    pred_intents_set = [set(i.values()) for i in intent_preds]\n",
    "    \n",
    "    mlb_intent = MultiLabelBinarizer(classes=ALL_INTENTS)\n",
    "    y_true_intents = mlb_intent.fit_transform([list(s['intents'].values()) for s in true_data])\n",
    "    y_pred_intents = mlb_intent.transform([list(i.values()) for i in intent_preds])\n",
    "    \n",
    "    intent_f1 = f1_score(y_true_intents, y_pred_intents, average='micro')\n",
    "    intent_precision = precision_score(y_true_intents, y_pred_intents, average='micro')\n",
    "    intent_recall = recall_score(y_true_intents, y_pred_intents, average='micro')\n",
    "    intent_exact_match = np.mean([t == p for t, p in zip(true_intents, pred_intents_set)])\n",
    "    \n",
    "    # NER metrics (if provided)\n",
    "    ner_f1 = 0.0\n",
    "    if ner_preds:\n",
    "        true_ner = [s['bio_tags'] for s in true_data]\n",
    "        aligned_true = []\n",
    "        aligned_pred = []\n",
    "        for t, p in zip(true_ner, ner_preds):\n",
    "            min_len = min(len(t), len(p))\n",
    "            aligned_true.append(t[:min_len])\n",
    "            aligned_pred.append(p[:min_len])\n",
    "        ner_f1 = seqeval_f1(aligned_true, aligned_pred, average='micro')\n",
    "    \n",
    "    combined_f1 = (tool_f1 + intent_f1 + ner_f1) / 3 if ner_preds else (tool_f1 + intent_f1) / 2\n",
    "    \n",
    "    return {\n",
    "        'tool_f1': tool_f1,\n",
    "        'tool_precision': tool_precision,\n",
    "        'tool_recall': tool_recall,\n",
    "        'tool_exact_match': tool_exact_match,\n",
    "        'intent_f1': intent_f1,\n",
    "        'intent_precision': intent_precision,\n",
    "        'intent_recall': intent_recall,\n",
    "        'intent_exact_match': intent_exact_match,\n",
    "        'ner_f1': ner_f1,\n",
    "        'combined_f1': combined_f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Evaluations\n",
    "\n",
    "**Note**: For cost efficiency, we'll evaluate on a subset of the test data for Claude Haiku 4.5. The full test set is used for DeBERTa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample size for Claude evaluation (to manage API costs)\n",
    "# Set to len(test_data) for full evaluation\n",
    "HAIKU_SAMPLE_SIZE = 500  # ~$0.50 in API costs\n",
    "\n",
    "# Use same sample for fair comparison\n",
    "np.random.seed(42)\n",
    "sample_indices = np.random.choice(len(test_data), size=min(HAIKU_SAMPLE_SIZE, len(test_data)), replace=False)\n",
    "sample_test_data = [test_data[i] for i in sample_indices]\n",
    "sample_queries = [s['query'] for s in sample_test_data]\n",
    "\n",
    "print(f\"Evaluating on {len(sample_test_data)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Claude Haiku 4.5\n",
    "print(\"\\nEvaluating Claude Haiku 4.5 baseline...\")\n",
    "print(\"This may take a while due to API rate limits...\")\n",
    "\n",
    "haiku_tools, haiku_intents, haiku_entities = haiku_baseline.predict_batch(\n",
    "    sample_queries, \n",
    "    batch_size=10,\n",
    "    delay=0.1  # 100ms delay between calls\n",
    ")\n",
    "\n",
    "haiku_stats = haiku_baseline.get_stats()\n",
    "haiku_metrics = compute_metrics(sample_test_data, haiku_tools, haiku_intents)\n",
    "\n",
    "print(f\"\\nClaude Haiku 4.5 Stats:\")\n",
    "print(f\"  API Calls: {haiku_stats['num_api_calls']}\")\n",
    "print(f\"  Total Tokens: {haiku_stats['total_input_tokens'] + haiku_stats['total_output_tokens']}\")\n",
    "print(f\"  Est. Cost: ${haiku_stats['total_cost_usd']:.4f}\")\n",
    "print(f\"  Avg Latency: {haiku_stats['avg_latency_ms']:.1f}ms per call\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate DeBERTa on same sample\n",
    "print(\"\\nEvaluating DeBERTa model on same sample...\")\n",
    "\n",
    "deberta_tools, deberta_intents, deberta_ner, deberta_latency = deberta_predict(\n",
    "    model, tokenizer, sample_queries, device\n",
    ")\n",
    "\n",
    "deberta_metrics = compute_metrics(sample_test_data, deberta_tools, deberta_intents, deberta_ner)\n",
    "\n",
    "print(f\"\\nDeBERTa Stats:\")\n",
    "print(f\"  Avg Latency: {deberta_latency:.2f}ms per query\")\n",
    "print(f\"  Cost: $0.00 (local inference)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "results = pd.DataFrame({\n",
    "    'Metric': [\n",
    "        'Tool F1', 'Tool Precision', 'Tool Recall', 'Tool Exact Match',\n",
    "        'Intent F1', 'Intent Precision', 'Intent Recall', 'Intent Exact Match',\n",
    "        'NER F1', 'Combined F1',\n",
    "        '---', \n",
    "        'Avg Latency (ms)', 'Est. Cost per 1K queries'\n",
    "    ],\n",
    "    'Claude Haiku 4.5': [\n",
    "        f\"{haiku_metrics['tool_f1']*100:.1f}%\",\n",
    "        f\"{haiku_metrics['tool_precision']*100:.1f}%\",\n",
    "        f\"{haiku_metrics['tool_recall']*100:.1f}%\",\n",
    "        f\"{haiku_metrics['tool_exact_match']*100:.1f}%\",\n",
    "        f\"{haiku_metrics['intent_f1']*100:.1f}%\",\n",
    "        f\"{haiku_metrics['intent_precision']*100:.1f}%\",\n",
    "        f\"{haiku_metrics['intent_recall']*100:.1f}%\",\n",
    "        f\"{haiku_metrics['intent_exact_match']*100:.1f}%\",\n",
    "        'N/A',\n",
    "        f\"{haiku_metrics['combined_f1']*100:.1f}%\",\n",
    "        '---',\n",
    "        f\"{haiku_stats['avg_latency_ms']:.0f}\",\n",
    "        f\"${haiku_stats['total_cost_usd'] / len(sample_queries) * 1000:.2f}\"\n",
    "    ],\n",
    "    'DeBERTa-v3 (Ours)': [\n",
    "        f\"{deberta_metrics['tool_f1']*100:.1f}%\",\n",
    "        f\"{deberta_metrics['tool_precision']*100:.1f}%\",\n",
    "        f\"{deberta_metrics['tool_recall']*100:.1f}%\",\n",
    "        f\"{deberta_metrics['tool_exact_match']*100:.1f}%\",\n",
    "        f\"{deberta_metrics['intent_f1']*100:.1f}%\",\n",
    "        f\"{deberta_metrics['intent_precision']*100:.1f}%\",\n",
    "        f\"{deberta_metrics['intent_recall']*100:.1f}%\",\n",
    "        f\"{deberta_metrics['intent_exact_match']*100:.1f}%\",\n",
    "        f\"{deberta_metrics['ner_f1']*100:.1f}%\",\n",
    "        f\"{deberta_metrics['combined_f1']*100:.1f}%\",\n",
    "        '---',\n",
    "        f\"{deberta_latency:.1f}\",\n",
    "        '$0.00'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATION RESULTS: Claude Haiku 4.5 vs DeBERTa-v3\")\n",
    "print(\"=\"*70)\n",
    "print(results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save results\nRESULTS_PATH.mkdir(exist_ok=True)\nresults.to_csv(RESULTS_PATH / 'evaluation_results.csv', index=False)\nprint(f\"\\nResults saved to {RESULTS_PATH / 'evaluation_results.csv'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Performance comparison chart\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nmodels = ['Claude Haiku 4.5', 'DeBERTa-v3']\ncolors = ['#FF6B6B', '#4ECDC4']\n\n# Tool F1\ntool_scores = [haiku_metrics['tool_f1'], deberta_metrics['tool_f1']]\nbars1 = axes[0].bar(models, tool_scores, color=colors)\naxes[0].set_ylabel('F1 Score')\naxes[0].set_title('Tool Classification F1')\naxes[0].set_ylim(0, 1)\nfor bar, score in zip(bars1, tool_scores):\n    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n                 f'{score:.2f}', ha='center', fontsize=12)\n\n# Intent F1\nintent_scores = [haiku_metrics['intent_f1'], deberta_metrics['intent_f1']]\nbars2 = axes[1].bar(models, intent_scores, color=colors)\naxes[1].set_ylabel('F1 Score')\naxes[1].set_title('Intent Classification F1')\naxes[1].set_ylim(0, 1)\nfor bar, score in zip(bars2, intent_scores):\n    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n                 f'{score:.2f}', ha='center', fontsize=12)\n\n# Latency comparison\nlatencies = [haiku_stats['avg_latency_ms'], deberta_latency]\nbars3 = axes[2].bar(models, latencies, color=colors)\naxes[2].set_ylabel('Latency (ms)')\naxes[2].set_title('Average Latency per Query')\nfor bar, lat in zip(bars3, latencies):\n    axes[2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n                 f'{lat:.0f}ms', ha='center', fontsize=12)\n\nplt.tight_layout()\nplt.savefig(RESULTS_PATH / 'model_comparison.png', dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cost comparison visualization\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Project costs for different query volumes\nquery_volumes = [100, 1000, 10000, 100000]\ncost_per_query_haiku = haiku_stats['total_cost_usd'] / len(sample_queries)\n\nhaiku_costs = [v * cost_per_query_haiku for v in query_volumes]\ndeberta_costs = [0] * len(query_volumes)  # Free after training\n\nx = np.arange(len(query_volumes))\nwidth = 0.35\n\nbars1 = ax.bar(x - width/2, haiku_costs, width, label='Claude Haiku 4.5', color='#FF6B6B')\nbars2 = ax.bar(x + width/2, deberta_costs, width, label='DeBERTa-v3 (Local)', color='#4ECDC4')\n\nax.set_ylabel('Cost (USD)')\nax.set_xlabel('Number of Queries')\nax.set_title('Inference Cost Comparison')\nax.set_xticks(x)\nax.set_xticklabels([f'{v:,}' for v in query_volumes])\nax.legend()\n\n# Add cost labels\nfor bar, cost in zip(bars1, haiku_costs):\n    if cost > 0:\n        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n                f'${cost:.2f}', ha='center', fontsize=9)\n\nplt.tight_layout()\nplt.savefig(RESULTS_PATH / 'cost_comparison.png', dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_errors(true_data, haiku_preds, deberta_preds):\n",
    "    \"\"\"Compare where each model makes errors.\"\"\"\n",
    "    haiku_correct = 0\n",
    "    deberta_correct = 0\n",
    "    both_correct = 0\n",
    "    both_wrong = 0\n",
    "    haiku_only = 0\n",
    "    deberta_only = 0\n",
    "    \n",
    "    disagreements = []\n",
    "    \n",
    "    for i, sample in enumerate(true_data):\n",
    "        true_tools = set(sample['tools'])\n",
    "        haiku_tools = set(haiku_preds[0][i])\n",
    "        deberta_tools = set(deberta_preds[0][i])\n",
    "        \n",
    "        haiku_match = true_tools == haiku_tools\n",
    "        deberta_match = true_tools == deberta_tools\n",
    "        \n",
    "        if haiku_match and deberta_match:\n",
    "            both_correct += 1\n",
    "        elif haiku_match and not deberta_match:\n",
    "            haiku_only += 1\n",
    "        elif deberta_match and not haiku_match:\n",
    "            deberta_only += 1\n",
    "        else:\n",
    "            both_wrong += 1\n",
    "            \n",
    "        if haiku_tools != deberta_tools:\n",
    "            disagreements.append({\n",
    "                'query': sample['query'],\n",
    "                'true': list(true_tools),\n",
    "                'haiku': list(haiku_tools),\n",
    "                'deberta': list(deberta_tools)\n",
    "            })\n",
    "    \n",
    "    return {\n",
    "        'both_correct': both_correct,\n",
    "        'haiku_only_correct': haiku_only,\n",
    "        'deberta_only_correct': deberta_only,\n",
    "        'both_wrong': both_wrong,\n",
    "        'disagreements': disagreements\n",
    "    }\n",
    "\n",
    "error_analysis = compare_errors(\n",
    "    sample_test_data,\n",
    "    (haiku_tools, haiku_intents),\n",
    "    (deberta_tools, deberta_intents)\n",
    ")\n",
    "\n",
    "print(\"\\nTool Prediction Agreement Analysis:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Both correct: {error_analysis['both_correct']} ({100*error_analysis['both_correct']/len(sample_test_data):.1f}%)\")\n",
    "print(f\"Haiku correct, DeBERTa wrong: {error_analysis['haiku_only_correct']} ({100*error_analysis['haiku_only_correct']/len(sample_test_data):.1f}%)\")\n",
    "print(f\"DeBERTa correct, Haiku wrong: {error_analysis['deberta_only_correct']} ({100*error_analysis['deberta_only_correct']/len(sample_test_data):.1f}%)\")\n",
    "print(f\"Both wrong: {error_analysis['both_wrong']} ({100*error_analysis['both_wrong']/len(sample_test_data):.1f}%)\")\n",
    "print(f\"\\nTotal disagreements: {len(error_analysis['disagreements'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample disagreements\n",
    "print(\"\\nSample Disagreements (where models predict different tools):\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for d in error_analysis['disagreements'][:10]:\n",
    "    print(f\"\\nQuery: {d['query'][:80]}...\")\n",
    "    print(f\"  True:    {d['true']}\")\n",
    "    print(f\"  Haiku:   {d['haiku']}\")\n",
    "    print(f\"  DeBERTa: {d['deberta']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n** Performance Comparison **\")\n",
    "print(\"-\"*50)\n",
    "print(f\"\\nClaude Haiku 4.5 (LLM Baseline):\")\n",
    "print(f\"  Tool F1:    {haiku_metrics['tool_f1']*100:.1f}%\")\n",
    "print(f\"  Intent F1:  {haiku_metrics['intent_f1']*100:.1f}%\")\n",
    "print(f\"  Combined:   {haiku_metrics['combined_f1']*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nDeBERTa-v3 (Our Model):\")\n",
    "print(f\"  Tool F1:    {deberta_metrics['tool_f1']*100:.1f}%\")\n",
    "print(f\"  Intent F1:  {deberta_metrics['intent_f1']*100:.1f}%\")\n",
    "print(f\"  NER F1:     {deberta_metrics['ner_f1']*100:.1f}%\")\n",
    "print(f\"  Combined:   {deberta_metrics['combined_f1']*100:.1f}%\")\n",
    "\n",
    "# Calculate improvements\n",
    "tool_improvement = (deberta_metrics['tool_f1'] - haiku_metrics['tool_f1']) * 100\n",
    "intent_improvement = (deberta_metrics['intent_f1'] - haiku_metrics['intent_f1']) * 100\n",
    "combined_improvement = (deberta_metrics['combined_f1'] - haiku_metrics['combined_f1']) * 100\n",
    "\n",
    "print(f\"\\n** Improvement over Baseline **\")\n",
    "print(\"-\"*50)\n",
    "print(f\"  Tool F1:    {tool_improvement:+.1f}%\")\n",
    "print(f\"  Intent F1:  {intent_improvement:+.1f}%\")\n",
    "print(f\"  Combined:   {combined_improvement:+.1f}%\")\n",
    "\n",
    "print(f\"\\n** Efficiency Comparison **\")\n",
    "print(\"-\"*50)\n",
    "speedup = haiku_stats['avg_latency_ms'] / deberta_latency\n",
    "print(f\"  Latency: {deberta_latency:.1f}ms vs {haiku_stats['avg_latency_ms']:.0f}ms ({speedup:.0f}x faster)\")\n",
    "print(f\"  Cost per 1K queries: $0.00 vs ${haiku_stats['total_cost_usd']/len(sample_queries)*1000:.2f}\")\n",
    "\n",
    "print(f\"\\n** Additional Capability **\")\n",
    "print(\"-\"*50)\n",
    "print(f\"  DeBERTa includes NER (F1: {deberta_metrics['ner_f1']*100:.1f}%) - not available in LLM baseline\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save complete evaluation report\nreport = {\n    'evaluation_date': time.strftime('%Y-%m-%d %H:%M:%S'),\n    'sample_size': len(sample_test_data),\n    'models': {\n        'baseline': {\n            'name': 'Claude Haiku 4.5',\n            'model_id': haiku_baseline.model,\n            'metrics': haiku_metrics,\n            'stats': haiku_stats\n        },\n        'our_model': {\n            'name': 'Joint DeBERTa-v3',\n            'model_id': CONFIG['model_name'],\n            'metrics': deberta_metrics,\n            'avg_latency_ms': deberta_latency\n        }\n    },\n    'improvements': {\n        'tool_f1': tool_improvement,\n        'intent_f1': intent_improvement,\n        'combined_f1': combined_improvement,\n        'speedup_factor': speedup\n    },\n    'error_analysis': {\n        'both_correct': error_analysis['both_correct'],\n        'haiku_only_correct': error_analysis['haiku_only_correct'],\n        'deberta_only_correct': error_analysis['deberta_only_correct'],\n        'both_wrong': error_analysis['both_wrong']\n    }\n}\n\nwith open(RESULTS_PATH / 'full_evaluation_report.json', 'w') as f:\n    json.dump(report, f, indent=2)\n\nprint(f\"\\nFull evaluation report saved to {RESULTS_PATH / 'full_evaluation_report.json'}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}