{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint DeBERTa-v3 Model for Intent Classification & NER\n",
    "\n",
    "This notebook implements a **single joint model** that performs both:\n",
    "1. **Multi-label Tool Classification** - Which tools to use (character_data, rulebook, session_notes)\n",
    "2. **Intent Classification** - What specific intent for each selected tool (61 total intents)\n",
    "3. **Named Entity Recognition (NER)** - Extract entities with BIO tagging\n",
    "\n",
    "## Architecture\n",
    "```\n",
    "                    DeBERTa-v3 Encoder\n",
    "                          │\n",
    "            ┌─────────────┼─────────────┐\n",
    "            │             │             │\n",
    "        [CLS] token   [CLS] token   All tokens\n",
    "            │             │             │\n",
    "      Tool Head     Intent Head    NER Head\n",
    "     (3 sigmoid)   (61 softmax)   (BIO tags)\n",
    "            │             │             │\n",
    "    Multi-label      Masked by      CRF Layer\n",
    "    BCE Loss       tool selection\n",
    "```\n",
    "\n",
    "## Why DeBERTa-v3?\n",
    "- **Disentangled attention** - better captures position and content separately\n",
    "- **Replaced Token Detection (RTD)** - more efficient pre-training than MLM\n",
    "- **0.9-3.6% improvement** over RoBERTa on NLU benchmarks\n",
    "- **Better data efficiency** - crucial for our ~10K synthetic samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers datasets torch accelerate seqeval scikit-learn wandb\n",
    "!pip install -q pytorch-crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set paths\n",
    "DRIVE_PATH = '/content/drive/MyDrive/574-assignment'\n",
    "DATA_PATH = f'{DRIVE_PATH}/data'\n",
    "MODEL_PATH = f'{DRIVE_PATH}/models/joint_deberta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    DebertaV2Tokenizer,\n",
    "    DebertaV2Model,\n",
    "    DebertaV2PreTrainedModel,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    AutoConfig\n",
    ")\n",
    "from torchcrf import CRF\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from seqeval.metrics import f1_score as seqeval_f1, classification_report\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "CONFIG = {\n",
    "    # Model - using deberta-v3-base for A100 (86M params)\n",
    "    # Alternatives: 'microsoft/deberta-v3-small' (44M), 'microsoft/deberta-v3-large' (304M)\n",
    "    'model_name': 'microsoft/deberta-v3-base',\n",
    "    \n",
    "    # Training\n",
    "    'batch_size': 32,  # Can increase on A100\n",
    "    'learning_rate': 2e-5,\n",
    "    'num_epochs': 10,\n",
    "    'warmup_ratio': 0.1,\n",
    "    'weight_decay': 0.01,\n",
    "    'max_length': 128,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    \n",
    "    # Loss weights (tune these)\n",
    "    'tool_loss_weight': 1.0,\n",
    "    'intent_loss_weight': 1.0,\n",
    "    'ner_loss_weight': 1.0,\n",
    "    \n",
    "    # Early stopping\n",
    "    'patience': 3,\n",
    "    \n",
    "    # Dropout\n",
    "    'classifier_dropout': 0.1,\n",
    "}\n",
    "\n",
    "# Tool and intent mappings\n",
    "TOOLS = ['character_data', 'rulebook', 'session_notes']\n",
    "TOOL_TO_IDX = {tool: idx for idx, tool in enumerate(TOOLS)}\n",
    "\n",
    "# Intent mappings per tool\n",
    "CHARACTER_INTENTS = [\n",
    "    'ability_scores', 'combat_info', 'skills_proficiencies', 'class_features',\n",
    "    'spell_slots', 'inventory', 'background', 'race_traits', 'level_info', 'full_character'\n",
    "]\n",
    "\n",
    "SESSION_INTENTS = [\n",
    "    'recent_events', 'npc_interactions', 'location_info', 'quest_status',\n",
    "    'party_decisions', 'combat_history', 'treasure_loot', 'plot_threads',\n",
    "    'character_development', 'time_tracking', 'relationship_status', 'faction_standing',\n",
    "    'unresolved_mysteries', 'player_notes', 'dm_notes', 'session_summary',\n",
    "    'next_session_hooks', 'world_lore', 'house_rules', 'campaign_timeline'\n",
    "]\n",
    "\n",
    "RULEBOOK_INTENTS = [\n",
    "    'spell_details', 'spell_list_query', 'class_info', 'subclass_info',\n",
    "    'race_info', 'feat_info', 'condition_rules', 'combat_rules',\n",
    "    'skill_rules', 'ability_check_rules', 'saving_throw_rules', 'death_rules',\n",
    "    'rest_rules', 'movement_rules', 'cover_rules', 'action_rules',\n",
    "    'reaction_rules', 'equipment_info', 'weapon_info', 'armor_info',\n",
    "    'magic_item_info', 'monster_info', 'multiclassing_rules', 'spellcasting_rules',\n",
    "    'concentration_rules', 'ritual_rules', 'component_rules', 'aoe_rules',\n",
    "    'range_rules', 'duration_rules', 'damage_type_rules'\n",
    "]\n",
    "\n",
    "ALL_INTENTS = CHARACTER_INTENTS + SESSION_INTENTS + RULEBOOK_INTENTS\n",
    "INTENT_TO_IDX = {intent: idx for idx, intent in enumerate(ALL_INTENTS)}\n",
    "IDX_TO_INTENT = {idx: intent for intent, idx in INTENT_TO_IDX.items()}\n",
    "\n",
    "# Intent to tool mapping\n",
    "INTENT_TO_TOOL = {}\n",
    "for intent in CHARACTER_INTENTS:\n",
    "    INTENT_TO_TOOL[intent] = 'character_data'\n",
    "for intent in SESSION_INTENTS:\n",
    "    INTENT_TO_TOOL[intent] = 'session_notes'\n",
    "for intent in RULEBOOK_INTENTS:\n",
    "    INTENT_TO_TOOL[intent] = 'rulebook'\n",
    "\n",
    "# NER tags\n",
    "NER_TAGS = ['O', 'B-SPELL', 'I-SPELL', 'B-CLASS', 'I-CLASS', 'B-RACE', 'I-RACE',\n",
    "            'B-CREATURE', 'I-CREATURE', 'B-ITEM', 'I-ITEM', 'B-LOCATION', 'I-LOCATION',\n",
    "            'B-ABILITY', 'I-ABILITY', 'B-SKILL', 'I-SKILL', 'B-CONDITION', 'I-CONDITION',\n",
    "            'B-DAMAGE_TYPE', 'I-DAMAGE_TYPE', 'B-FEAT', 'I-FEAT', 'B-BACKGROUND', 'I-BACKGROUND']\n",
    "TAG_TO_IDX = {tag: idx for idx, tag in enumerate(NER_TAGS)}\n",
    "IDX_TO_TAG = {idx: tag for tag, idx in TAG_TO_IDX.items()}\n",
    "\n",
    "print(f\"Number of tools: {len(TOOLS)}\")\n",
    "print(f\"Number of intents: {len(ALL_INTENTS)}\")\n",
    "print(f\"Number of NER tags: {len(NER_TAGS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(split='train'):\n",
    "    \"\"\"Load dataset from JSON file.\"\"\"\n",
    "    path = f'{DATA_PATH}/{split}.json'\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"Loaded {len(data)} {split} samples\")\n",
    "    return data\n",
    "\n",
    "train_data = load_dataset('train')\n",
    "val_data = load_dataset('val')\n",
    "test_data = load_dataset('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine sample structure\n",
    "print(\"Sample structure:\")\n",
    "print(json.dumps(train_data[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointDataset(Dataset):\n",
    "    \"\"\"Dataset for joint tool/intent classification and NER.\"\"\"\n",
    "    \n",
    "    def __init__(self, data, tokenizer, max_length=128):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        text = sample['query']\n",
    "        tools = sample['tools']\n",
    "        intents = sample['intents']\n",
    "        bio_tags = sample['bio_tags']\n",
    "        \n",
    "        # Tokenize with word IDs for aligning BIO tags\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "            return_offsets_mapping=True,\n",
    "            return_special_tokens_mask=True\n",
    "        )\n",
    "        \n",
    "        # Tool labels (multi-label)\n",
    "        tool_labels = torch.zeros(len(TOOLS))\n",
    "        for tool in tools:\n",
    "            if tool in TOOL_TO_IDX:\n",
    "                tool_labels[TOOL_TO_IDX[tool]] = 1\n",
    "        \n",
    "        # Intent labels - multi-label for all selected intents\n",
    "        intent_labels = torch.zeros(len(ALL_INTENTS))\n",
    "        for tool, intent in intents.items():\n",
    "            if intent in INTENT_TO_IDX:\n",
    "                intent_labels[INTENT_TO_IDX[intent]] = 1\n",
    "        \n",
    "        # Align BIO tags with tokens\n",
    "        ner_labels = self._align_labels(\n",
    "            text, bio_tags, encoding, encoding['offset_mapping'][0]\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'tool_labels': tool_labels,\n",
    "            'intent_labels': intent_labels,\n",
    "            'ner_labels': ner_labels,\n",
    "            'tools': tools,  # Keep for evaluation\n",
    "            'intents': intents\n",
    "        }\n",
    "    \n",
    "    def _align_labels(self, text, bio_tags, encoding, offset_mapping):\n",
    "        \"\"\"Align word-level BIO tags to subword tokens.\"\"\"\n",
    "        # Split text into words\n",
    "        words = text.split()\n",
    "        \n",
    "        # Handle mismatch between words and bio_tags\n",
    "        if len(bio_tags) != len(words):\n",
    "            # Pad or truncate bio_tags\n",
    "            if len(bio_tags) < len(words):\n",
    "                bio_tags = bio_tags + ['O'] * (len(words) - len(bio_tags))\n",
    "            else:\n",
    "                bio_tags = bio_tags[:len(words)]\n",
    "        \n",
    "        # Create word boundaries\n",
    "        word_boundaries = []\n",
    "        current_pos = 0\n",
    "        for word in words:\n",
    "            start = text.find(word, current_pos)\n",
    "            if start == -1:\n",
    "                start = current_pos\n",
    "            end = start + len(word)\n",
    "            word_boundaries.append((start, end))\n",
    "            current_pos = end\n",
    "        \n",
    "        # Map each token to a word\n",
    "        aligned_labels = []\n",
    "        special_tokens_mask = encoding['special_tokens_mask'][0].tolist()\n",
    "        \n",
    "        for idx, (start, end) in enumerate(offset_mapping.tolist()):\n",
    "            if special_tokens_mask[idx] or (start == 0 and end == 0):\n",
    "                # Special token or padding - use -100 (ignored in loss)\n",
    "                aligned_labels.append(-100)\n",
    "            else:\n",
    "                # Find which word this token belongs to\n",
    "                word_idx = None\n",
    "                for w_idx, (w_start, w_end) in enumerate(word_boundaries):\n",
    "                    if start >= w_start and start < w_end:\n",
    "                        word_idx = w_idx\n",
    "                        break\n",
    "                \n",
    "                if word_idx is not None and word_idx < len(bio_tags):\n",
    "                    tag = bio_tags[word_idx]\n",
    "                    # If this is a continuation token (not first subword), convert B- to I-\n",
    "                    if start > word_boundaries[word_idx][0] and tag.startswith('B-'):\n",
    "                        tag = 'I-' + tag[2:]\n",
    "                    aligned_labels.append(TAG_TO_IDX.get(tag, TAG_TO_IDX['O']))\n",
    "                else:\n",
    "                    aligned_labels.append(TAG_TO_IDX['O'])\n",
    "        \n",
    "        return torch.tensor(aligned_labels, dtype=torch.long)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function.\"\"\"\n",
    "    return {\n",
    "        'input_ids': torch.stack([x['input_ids'] for x in batch]),\n",
    "        'attention_mask': torch.stack([x['attention_mask'] for x in batch]),\n",
    "        'tool_labels': torch.stack([x['tool_labels'] for x in batch]),\n",
    "        'intent_labels': torch.stack([x['intent_labels'] for x in batch]),\n",
    "        'ner_labels': torch.stack([x['ner_labels'] for x in batch]),\n",
    "        'tools': [x['tools'] for x in batch],\n",
    "        'intents': [x['intents'] for x in batch]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Joint Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointDeBERTaModel(DebertaV2PreTrainedModel):\n",
    "    \"\"\"\n",
    "    Joint DeBERTa-v3 model for:\n",
    "    1. Multi-label tool classification (from [CLS])\n",
    "    2. Multi-label intent classification (from [CLS])\n",
    "    3. NER/slot filling with CRF (from all tokens)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config, num_tools=3, num_intents=61, num_ner_tags=25):\n",
    "        super().__init__(config)\n",
    "        \n",
    "        self.num_tools = num_tools\n",
    "        self.num_intents = num_intents\n",
    "        self.num_ner_tags = num_ner_tags\n",
    "        \n",
    "        # Shared encoder\n",
    "        self.deberta = DebertaV2Model(config)\n",
    "        \n",
    "        # Dropout\n",
    "        classifier_dropout = (\n",
    "            config.classifier_dropout \n",
    "            if hasattr(config, 'classifier_dropout') and config.classifier_dropout is not None\n",
    "            else config.hidden_dropout_prob\n",
    "        )\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        \n",
    "        # Tool classification head (multi-label)\n",
    "        self.tool_classifier = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(config.hidden_size, num_tools)\n",
    "        )\n",
    "        \n",
    "        # Intent classification head (multi-label)\n",
    "        self.intent_classifier = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(config.hidden_size, num_intents)\n",
    "        )\n",
    "        \n",
    "        # NER head with CRF\n",
    "        self.ner_classifier = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.hidden_size // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(classifier_dropout),\n",
    "            nn.Linear(config.hidden_size // 2, num_ner_tags)\n",
    "        )\n",
    "        self.crf = CRF(num_ner_tags, batch_first=True)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.post_init()\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        tool_labels=None,\n",
    "        intent_labels=None,\n",
    "        ner_labels=None,\n",
    "        return_dict=True\n",
    "    ):\n",
    "        # Get encoder outputs\n",
    "        outputs = self.deberta(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        sequence_output = outputs.last_hidden_state  # (batch, seq_len, hidden)\n",
    "        cls_output = sequence_output[:, 0, :]  # (batch, hidden)\n",
    "        cls_output = self.dropout(cls_output)\n",
    "        \n",
    "        # Tool classification\n",
    "        tool_logits = self.tool_classifier(cls_output)  # (batch, num_tools)\n",
    "        \n",
    "        # Intent classification\n",
    "        intent_logits = self.intent_classifier(cls_output)  # (batch, num_intents)\n",
    "        \n",
    "        # NER classification\n",
    "        ner_emissions = self.ner_classifier(sequence_output)  # (batch, seq_len, num_tags)\n",
    "        \n",
    "        # Compute losses if labels provided\n",
    "        loss = None\n",
    "        tool_loss = None\n",
    "        intent_loss = None\n",
    "        ner_loss = None\n",
    "        \n",
    "        if tool_labels is not None:\n",
    "            tool_loss = F.binary_cross_entropy_with_logits(tool_logits, tool_labels)\n",
    "        \n",
    "        if intent_labels is not None:\n",
    "            intent_loss = F.binary_cross_entropy_with_logits(intent_logits, intent_labels)\n",
    "        \n",
    "        if ner_labels is not None:\n",
    "            # Create mask for valid tokens (not -100)\n",
    "            valid_mask = (ner_labels != -100)\n",
    "            # Replace -100 with 0 for CRF (will be masked anyway)\n",
    "            ner_labels_clean = ner_labels.clone()\n",
    "            ner_labels_clean[~valid_mask] = 0\n",
    "            # CRF loss (negative log likelihood)\n",
    "            ner_loss = -self.crf(ner_emissions, ner_labels_clean, mask=valid_mask, reduction='mean')\n",
    "        \n",
    "        if tool_loss is not None and intent_loss is not None and ner_loss is not None:\n",
    "            loss = (\n",
    "                CONFIG['tool_loss_weight'] * tool_loss +\n",
    "                CONFIG['intent_loss_weight'] * intent_loss +\n",
    "                CONFIG['ner_loss_weight'] * ner_loss\n",
    "            )\n",
    "        \n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'tool_loss': tool_loss,\n",
    "            'intent_loss': intent_loss,\n",
    "            'ner_loss': ner_loss,\n",
    "            'tool_logits': tool_logits,\n",
    "            'intent_logits': intent_logits,\n",
    "            'ner_emissions': ner_emissions,\n",
    "        }\n",
    "    \n",
    "    def decode_ner(self, ner_emissions, attention_mask):\n",
    "        \"\"\"Decode NER predictions using CRF.\"\"\"\n",
    "        return self.crf.decode(ner_emissions, mask=attention_mask.bool())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and config\n",
    "print(f\"Loading model: {CONFIG['model_name']}\")\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(CONFIG['model_name'])\n",
    "config = AutoConfig.from_pretrained(CONFIG['model_name'])\n",
    "config.classifier_dropout = CONFIG['classifier_dropout']\n",
    "\n",
    "# Initialize model\n",
    "model = JointDeBERTaModel.from_pretrained(\n",
    "    CONFIG['model_name'],\n",
    "    config=config,\n",
    "    num_tools=len(TOOLS),\n",
    "    num_intents=len(ALL_INTENTS),\n",
    "    num_ner_tags=len(NER_TAGS),\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model size\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_dataset = JointDataset(train_data, tokenizer, CONFIG['max_length'])\n",
    "val_dataset = JointDataset(val_data, tokenizer, CONFIG['max_length'])\n",
    "test_dataset = JointDataset(test_data, tokenizer, CONFIG['max_length'])\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=CONFIG['batch_size'], \n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer with layer-wise learning rate decay\n",
    "def get_optimizer_grouped_parameters(model, learning_rate, weight_decay):\n",
    "    \"\"\"Group parameters with different learning rates.\"\"\"\n",
    "    no_decay = ['bias', 'LayerNorm.weight', 'layernorm.weight']\n",
    "    \n",
    "    # Different learning rates for encoder vs classifiers\n",
    "    encoder_params = []\n",
    "    classifier_params = []\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'deberta' in name:\n",
    "            encoder_params.append((name, param))\n",
    "        else:\n",
    "            classifier_params.append((name, param))\n",
    "    \n",
    "    optimizer_grouped_parameters = [\n",
    "        # Encoder with weight decay\n",
    "        {\n",
    "            'params': [p for n, p in encoder_params if not any(nd in n for nd in no_decay)],\n",
    "            'lr': learning_rate,\n",
    "            'weight_decay': weight_decay\n",
    "        },\n",
    "        # Encoder without weight decay\n",
    "        {\n",
    "            'params': [p for n, p in encoder_params if any(nd in n for nd in no_decay)],\n",
    "            'lr': learning_rate,\n",
    "            'weight_decay': 0.0\n",
    "        },\n",
    "        # Classifiers with higher learning rate\n",
    "        {\n",
    "            'params': [p for n, p in classifier_params if not any(nd in n for nd in no_decay)],\n",
    "            'lr': learning_rate * 10,  # Higher LR for classification heads\n",
    "            'weight_decay': weight_decay\n",
    "        },\n",
    "        {\n",
    "            'params': [p for n, p in classifier_params if any(nd in n for nd in no_decay)],\n",
    "            'lr': learning_rate * 10,\n",
    "            'weight_decay': 0.0\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    return optimizer_grouped_parameters\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    get_optimizer_grouped_parameters(model, CONFIG['learning_rate'], CONFIG['weight_decay'])\n",
    ")\n",
    "\n",
    "# Learning rate scheduler with warmup\n",
    "total_steps = len(train_loader) * CONFIG['num_epochs'] // CONFIG['gradient_accumulation_steps']\n",
    "warmup_steps = int(total_steps * CONFIG['warmup_ratio'])\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"Total training steps: {total_steps}\")\n",
    "print(f\"Warmup steps: {warmup_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device):\n",
    "    \"\"\"Evaluate model on validation/test set.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_tool_preds = []\n",
    "    all_tool_labels = []\n",
    "    all_intent_preds = []\n",
    "    all_intent_labels = []\n",
    "    all_ner_preds = []\n",
    "    all_ner_labels = []\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_tool_loss = 0\n",
    "    total_intent_loss = 0\n",
    "    total_ner_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            tool_labels = batch['tool_labels'].to(device)\n",
    "            intent_labels = batch['intent_labels'].to(device)\n",
    "            ner_labels = batch['ner_labels'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                tool_labels=tool_labels,\n",
    "                intent_labels=intent_labels,\n",
    "                ner_labels=ner_labels\n",
    "            )\n",
    "            \n",
    "            total_loss += outputs['loss'].item()\n",
    "            total_tool_loss += outputs['tool_loss'].item()\n",
    "            total_intent_loss += outputs['intent_loss'].item()\n",
    "            total_ner_loss += outputs['ner_loss'].item()\n",
    "            \n",
    "            # Tool predictions (threshold 0.5)\n",
    "            tool_preds = (torch.sigmoid(outputs['tool_logits']) > 0.5).float()\n",
    "            all_tool_preds.append(tool_preds.cpu())\n",
    "            all_tool_labels.append(tool_labels.cpu())\n",
    "            \n",
    "            # Intent predictions (threshold 0.5 for multi-label)\n",
    "            intent_preds = (torch.sigmoid(outputs['intent_logits']) > 0.5).float()\n",
    "            all_intent_preds.append(intent_preds.cpu())\n",
    "            all_intent_labels.append(intent_labels.cpu())\n",
    "            \n",
    "            # NER predictions (CRF decode)\n",
    "            ner_preds = model.decode_ner(outputs['ner_emissions'], attention_mask)\n",
    "            \n",
    "            # Convert to tag sequences for seqeval\n",
    "            for i, (pred_seq, label_seq) in enumerate(zip(ner_preds, ner_labels.cpu().tolist())):\n",
    "                pred_tags = []\n",
    "                label_tags = []\n",
    "                for pred_tag, label_tag in zip(pred_seq, label_seq):\n",
    "                    if label_tag != -100:  # Only consider valid tokens\n",
    "                        pred_tags.append(IDX_TO_TAG[pred_tag])\n",
    "                        label_tags.append(IDX_TO_TAG[label_tag])\n",
    "                if pred_tags:\n",
    "                    all_ner_preds.append(pred_tags)\n",
    "                    all_ner_labels.append(label_tags)\n",
    "    \n",
    "    # Compute metrics\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    # Tool metrics\n",
    "    all_tool_preds = torch.cat(all_tool_preds, dim=0).numpy()\n",
    "    all_tool_labels = torch.cat(all_tool_labels, dim=0).numpy()\n",
    "    tool_f1 = f1_score(all_tool_labels, all_tool_preds, average='micro', zero_division=0)\n",
    "    tool_precision = precision_score(all_tool_labels, all_tool_preds, average='micro', zero_division=0)\n",
    "    tool_recall = recall_score(all_tool_labels, all_tool_preds, average='micro', zero_division=0)\n",
    "    \n",
    "    # Exact match for tools\n",
    "    tool_exact_match = np.mean(np.all(all_tool_preds == all_tool_labels, axis=1))\n",
    "    \n",
    "    # Intent metrics\n",
    "    all_intent_preds = torch.cat(all_intent_preds, dim=0).numpy()\n",
    "    all_intent_labels = torch.cat(all_intent_labels, dim=0).numpy()\n",
    "    intent_f1 = f1_score(all_intent_labels, all_intent_preds, average='micro', zero_division=0)\n",
    "    intent_precision = precision_score(all_intent_labels, all_intent_preds, average='micro', zero_division=0)\n",
    "    intent_recall = recall_score(all_intent_labels, all_intent_preds, average='micro', zero_division=0)\n",
    "    \n",
    "    # Exact match for intents\n",
    "    intent_exact_match = np.mean(np.all(all_intent_preds == all_intent_labels, axis=1))\n",
    "    \n",
    "    # NER metrics (using seqeval)\n",
    "    ner_f1 = seqeval_f1(all_ner_labels, all_ner_preds, average='micro', zero_division=0)\n",
    "    \n",
    "    metrics = {\n",
    "        'loss': total_loss / num_batches,\n",
    "        'tool_loss': total_tool_loss / num_batches,\n",
    "        'intent_loss': total_intent_loss / num_batches,\n",
    "        'ner_loss': total_ner_loss / num_batches,\n",
    "        'tool_f1': tool_f1,\n",
    "        'tool_precision': tool_precision,\n",
    "        'tool_recall': tool_recall,\n",
    "        'tool_exact_match': tool_exact_match,\n",
    "        'intent_f1': intent_f1,\n",
    "        'intent_precision': intent_precision,\n",
    "        'intent_recall': intent_recall,\n",
    "        'intent_exact_match': intent_exact_match,\n",
    "        'ner_f1': ner_f1,\n",
    "    }\n",
    "    \n",
    "    return metrics, (all_ner_labels, all_ner_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, scheduler, device, accumulation_steps=1):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_tool_loss = 0\n",
    "    total_intent_loss = 0\n",
    "    total_ner_loss = 0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        tool_labels = batch['tool_labels'].to(device)\n",
    "        intent_labels = batch['intent_labels'].to(device)\n",
    "        ner_labels = batch['ner_labels'].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            tool_labels=tool_labels,\n",
    "            intent_labels=intent_labels,\n",
    "            ner_labels=ner_labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs['loss'] / accumulation_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        if (step + 1) % accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += outputs['loss'].item()\n",
    "        total_tool_loss += outputs['tool_loss'].item()\n",
    "        total_intent_loss += outputs['intent_loss'].item()\n",
    "        total_ner_loss += outputs['ner_loss'].item()\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f\"{outputs['loss'].item():.4f}\",\n",
    "            'tool': f\"{outputs['tool_loss'].item():.4f}\",\n",
    "            'intent': f\"{outputs['intent_loss'].item():.4f}\",\n",
    "            'ner': f\"{outputs['ner_loss'].item():.4f}\"\n",
    "        })\n",
    "    \n",
    "    num_batches = len(dataloader)\n",
    "    return {\n",
    "        'loss': total_loss / num_batches,\n",
    "        'tool_loss': total_tool_loss / num_batches,\n",
    "        'intent_loss': total_intent_loss / num_batches,\n",
    "        'ner_loss': total_ner_loss / num_batches\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with early stopping\n",
    "best_val_f1 = 0\n",
    "patience_counter = 0\n",
    "training_history = []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(CONFIG['num_epochs']):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{CONFIG['num_epochs']}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Train\n",
    "    train_metrics = train_epoch(\n",
    "        model, train_loader, optimizer, scheduler, device, \n",
    "        CONFIG['gradient_accumulation_steps']\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    val_metrics, _ = evaluate(model, val_loader, device)\n",
    "    \n",
    "    # Combined F1 score (average of all three tasks)\n",
    "    combined_f1 = (val_metrics['tool_f1'] + val_metrics['intent_f1'] + val_metrics['ner_f1']) / 3\n",
    "    \n",
    "    # Log metrics\n",
    "    history_entry = {\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': train_metrics['loss'],\n",
    "        'val_loss': val_metrics['loss'],\n",
    "        'val_tool_f1': val_metrics['tool_f1'],\n",
    "        'val_tool_exact_match': val_metrics['tool_exact_match'],\n",
    "        'val_intent_f1': val_metrics['intent_f1'],\n",
    "        'val_intent_exact_match': val_metrics['intent_exact_match'],\n",
    "        'val_ner_f1': val_metrics['ner_f1'],\n",
    "        'val_combined_f1': combined_f1\n",
    "    }\n",
    "    training_history.append(history_entry)\n",
    "    \n",
    "    print(f\"\\nTrain Loss: {train_metrics['loss']:.4f}\")\n",
    "    print(f\"Val Loss: {val_metrics['loss']:.4f}\")\n",
    "    print(f\"Val Tool F1: {val_metrics['tool_f1']:.4f} (Exact Match: {val_metrics['tool_exact_match']:.4f})\")\n",
    "    print(f\"Val Intent F1: {val_metrics['intent_f1']:.4f} (Exact Match: {val_metrics['intent_exact_match']:.4f})\")\n",
    "    print(f\"Val NER F1: {val_metrics['ner_f1']:.4f}\")\n",
    "    print(f\"Combined F1: {combined_f1:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if combined_f1 > best_val_f1:\n",
    "        best_val_f1 = combined_f1\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Save model\n",
    "        os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "        model.save_pretrained(MODEL_PATH)\n",
    "        tokenizer.save_pretrained(MODEL_PATH)\n",
    "        \n",
    "        # Save config\n",
    "        with open(f'{MODEL_PATH}/training_config.json', 'w') as f:\n",
    "            json.dump(CONFIG, f, indent=2)\n",
    "        \n",
    "        # Save label mappings\n",
    "        mappings = {\n",
    "            'tool_to_idx': TOOL_TO_IDX,\n",
    "            'intent_to_idx': INTENT_TO_IDX,\n",
    "            'tag_to_idx': TAG_TO_IDX,\n",
    "            'intent_to_tool': INTENT_TO_TOOL\n",
    "        }\n",
    "        with open(f'{MODEL_PATH}/label_mappings.json', 'w') as f:\n",
    "            json.dump(mappings, f, indent=2)\n",
    "        \n",
    "        print(f\"\\n✓ New best model saved! Combined F1: {combined_f1:.4f}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"\\n✗ No improvement. Patience: {patience_counter}/{CONFIG['patience']}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= CONFIG['patience']:\n",
    "        print(f\"\\nEarly stopping triggered after {epoch + 1} epochs\")\n",
    "        break\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"Training complete! Best Combined F1: {best_val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for testing\n",
    "print(\"Loading best model for final evaluation...\")\n",
    "model = JointDeBERTaModel.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    num_tools=len(TOOLS),\n",
    "    num_intents=len(ALL_INTENTS),\n",
    "    num_ner_tags=len(NER_TAGS)\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_metrics, (ner_labels, ner_preds) = evaluate(model, test_loader, device)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTool Classification:\")\n",
    "print(f\"  F1 Score: {test_metrics['tool_f1']:.4f}\")\n",
    "print(f\"  Precision: {test_metrics['tool_precision']:.4f}\")\n",
    "print(f\"  Recall: {test_metrics['tool_recall']:.4f}\")\n",
    "print(f\"  Exact Match: {test_metrics['tool_exact_match']:.4f}\")\n",
    "\n",
    "print(f\"\\nIntent Classification:\")\n",
    "print(f\"  F1 Score: {test_metrics['intent_f1']:.4f}\")\n",
    "print(f\"  Precision: {test_metrics['intent_precision']:.4f}\")\n",
    "print(f\"  Recall: {test_metrics['intent_recall']:.4f}\")\n",
    "print(f\"  Exact Match: {test_metrics['intent_exact_match']:.4f}\")\n",
    "\n",
    "print(f\"\\nNER (Entity Extraction):\")\n",
    "print(f\"  F1 Score: {test_metrics['ner_f1']:.4f}\")\n",
    "\n",
    "# Detailed NER report\n",
    "print(\"\\nDetailed NER Classification Report:\")\n",
    "print(classification_report(ner_labels, ner_preds, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test results\n",
    "test_results = {\n",
    "    'model_name': CONFIG['model_name'],\n",
    "    'test_metrics': test_metrics,\n",
    "    'training_history': training_history,\n",
    "    'best_val_f1': best_val_f1,\n",
    "    'config': CONFIG\n",
    "}\n",
    "\n",
    "with open(f'{MODEL_PATH}/test_results.json', 'w') as f:\n",
    "    json.dump(test_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nTest results saved to {MODEL_PATH}/test_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "epochs = [h['epoch'] for h in training_history]\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(epochs, [h['train_loss'] for h in training_history], 'b-', label='Train')\n",
    "axes[0, 0].plot(epochs, [h['val_loss'] for h in training_history], 'r-', label='Validation')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training & Validation Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Tool F1\n",
    "axes[0, 1].plot(epochs, [h['val_tool_f1'] for h in training_history], 'g-', marker='o')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('F1 Score')\n",
    "axes[0, 1].set_title('Tool Classification F1')\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Intent F1\n",
    "axes[1, 0].plot(epochs, [h['val_intent_f1'] for h in training_history], 'm-', marker='o')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('F1 Score')\n",
    "axes[1, 0].set_title('Intent Classification F1')\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# NER F1\n",
    "axes[1, 1].plot(epochs, [h['val_ner_f1'] for h in training_history], 'c-', marker='o')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('F1 Score')\n",
    "axes[1, 1].set_title('NER F1 Score')\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{MODEL_PATH}/training_history.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Inference Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, tokenizer, text, device):\n",
    "    \"\"\"Run inference on a single query.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        max_length=CONFIG['max_length'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    # Tool predictions\n",
    "    tool_probs = torch.sigmoid(outputs['tool_logits']).cpu().numpy()[0]\n",
    "    predicted_tools = [TOOLS[i] for i, p in enumerate(tool_probs) if p > 0.5]\n",
    "    \n",
    "    # Intent predictions\n",
    "    intent_probs = torch.sigmoid(outputs['intent_logits']).cpu().numpy()[0]\n",
    "    predicted_intents = {}\n",
    "    for tool in predicted_tools:\n",
    "        # Get intents for this tool\n",
    "        tool_intents = [intent for intent, t in INTENT_TO_TOOL.items() if t == tool]\n",
    "        tool_intent_probs = [(intent, intent_probs[INTENT_TO_IDX[intent]]) \n",
    "                             for intent in tool_intents]\n",
    "        best_intent = max(tool_intent_probs, key=lambda x: x[1])\n",
    "        predicted_intents[tool] = best_intent[0]\n",
    "    \n",
    "    # NER predictions\n",
    "    ner_preds = model.decode_ner(outputs['ner_emissions'], attention_mask)[0]\n",
    "    \n",
    "    # Extract entities\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    entities = []\n",
    "    current_entity = None\n",
    "    current_type = None\n",
    "    \n",
    "    for i, (token, tag_idx) in enumerate(zip(tokens, ner_preds)):\n",
    "        if token in ['[CLS]', '[SEP]', '[PAD]', '<s>', '</s>', '<pad>']:\n",
    "            continue\n",
    "        \n",
    "        tag = IDX_TO_TAG[tag_idx]\n",
    "        \n",
    "        if tag.startswith('B-'):\n",
    "            if current_entity:\n",
    "                entities.append({'text': current_entity, 'type': current_type})\n",
    "            current_entity = token.replace('▁', ' ').strip()\n",
    "            current_type = tag[2:]\n",
    "        elif tag.startswith('I-') and current_type == tag[2:]:\n",
    "            current_entity += token.replace('▁', ' ')\n",
    "        else:\n",
    "            if current_entity:\n",
    "                entities.append({'text': current_entity, 'type': current_type})\n",
    "            current_entity = None\n",
    "            current_type = None\n",
    "    \n",
    "    if current_entity:\n",
    "        entities.append({'text': current_entity, 'type': current_type})\n",
    "    \n",
    "    return {\n",
    "        'tools': predicted_tools,\n",
    "        'tool_probs': {TOOLS[i]: float(p) for i, p in enumerate(tool_probs)},\n",
    "        'intents': predicted_intents,\n",
    "        'entities': entities\n",
    "    }\n",
    "\n",
    "\n",
    "# Test with example queries\n",
    "test_queries = [\n",
    "    \"What's my armor class and how does Shield work?\",\n",
    "    \"Did we meet anyone named the Archmage in our last session?\",\n",
    "    \"How much damage does Fireball do and can I cast it at 5th level?\",\n",
    "    \"What are my spell slots and did we find any magic items last time?\"\n",
    "]\n",
    "\n",
    "print(\"\\nExample Predictions:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for query in test_queries:\n",
    "    result = predict(model, tokenizer, query, device)\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(f\"Tools: {result['tools']}\")\n",
    "    print(f\"Tool probabilities: {result['tool_probs']}\")\n",
    "    print(f\"Intents: {result['intents']}\")\n",
    "    print(f\"Entities: {result['entities']}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Export Model for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a standalone inference class\n",
    "inference_code = '''\n",
    "import json\n",
    "import torch\n",
    "from transformers import DebertaV2Tokenizer, DebertaV2Model, DebertaV2PreTrainedModel\n",
    "from torchcrf import CRF\n",
    "import torch.nn as nn\n",
    "\n",
    "class JointDeBERTaInference:\n",
    "    \"\"\"Inference wrapper for the joint DeBERTa model.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path, device='cuda'):\n",
    "        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Load mappings\n",
    "        with open(f'{model_path}/label_mappings.json') as f:\n",
    "            mappings = json.load(f)\n",
    "        \n",
    "        self.tool_to_idx = mappings['tool_to_idx']\n",
    "        self.idx_to_tool = {int(v): k for k, v in self.tool_to_idx.items()}\n",
    "        self.intent_to_idx = mappings['intent_to_idx']\n",
    "        self.idx_to_intent = {int(v): k for k, v in self.intent_to_idx.items()}\n",
    "        self.tag_to_idx = mappings['tag_to_idx']\n",
    "        self.idx_to_tag = {int(v): k for k, v in self.tag_to_idx.items()}\n",
    "        self.intent_to_tool = mappings['intent_to_tool']\n",
    "        \n",
    "        # Load config\n",
    "        with open(f'{model_path}/training_config.json') as f:\n",
    "            self.config = json.load(f)\n",
    "        \n",
    "        # Load model and tokenizer\n",
    "        self.tokenizer = DebertaV2Tokenizer.from_pretrained(model_path)\n",
    "        self.model = JointDeBERTaModel.from_pretrained(\n",
    "            model_path,\n",
    "            num_tools=len(self.tool_to_idx),\n",
    "            num_intents=len(self.intent_to_idx),\n",
    "            num_ner_tags=len(self.tag_to_idx)\n",
    "        )\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "    \n",
    "    def predict(self, text):\n",
    "        \"\"\"Run inference on a query.\"\"\"\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.config['max_length'],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].to(self.device)\n",
    "        attention_mask = encoding['attention_mask'].to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Parse predictions\n",
    "        tool_probs = torch.sigmoid(outputs['tool_logits']).cpu().numpy()[0]\n",
    "        predicted_tools = [self.idx_to_tool[i] for i, p in enumerate(tool_probs) if p > 0.5]\n",
    "        \n",
    "        intent_probs = torch.sigmoid(outputs['intent_logits']).cpu().numpy()[0]\n",
    "        predicted_intents = {}\n",
    "        for tool in predicted_tools:\n",
    "            tool_intents = [intent for intent, t in self.intent_to_tool.items() if t == tool]\n",
    "            tool_intent_probs = [(intent, intent_probs[self.intent_to_idx[intent]]) \n",
    "                                 for intent in tool_intents]\n",
    "            best_intent = max(tool_intent_probs, key=lambda x: x[1])\n",
    "            predicted_intents[tool] = best_intent[0]\n",
    "        \n",
    "        # NER\n",
    "        ner_preds = self.model.decode_ner(outputs['ner_emissions'], attention_mask)[0]\n",
    "        entities = self._extract_entities(input_ids[0], ner_preds)\n",
    "        \n",
    "        return {\n",
    "            'tools': predicted_tools,\n",
    "            'intents': predicted_intents,\n",
    "            'entities': entities\n",
    "        }\n",
    "    \n",
    "    def _extract_entities(self, input_ids, ner_preds):\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(input_ids)\n",
    "        entities = []\n",
    "        current_entity = None\n",
    "        current_type = None\n",
    "        \n",
    "        for token, tag_idx in zip(tokens, ner_preds):\n",
    "            if token in ['[CLS]', '[SEP]', '[PAD]', '<s>', '</s>', '<pad>']:\n",
    "                continue\n",
    "            tag = self.idx_to_tag[tag_idx]\n",
    "            if tag.startswith('B-'):\n",
    "                if current_entity:\n",
    "                    entities.append({'text': current_entity.strip(), 'type': current_type})\n",
    "                current_entity = token.replace('▁', ' ')\n",
    "                current_type = tag[2:]\n",
    "            elif tag.startswith('I-') and current_type == tag[2:]:\n",
    "                current_entity += token.replace('▁', ' ')\n",
    "            else:\n",
    "                if current_entity:\n",
    "                    entities.append({'text': current_entity.strip(), 'type': current_type})\n",
    "                current_entity = None\n",
    "                current_type = None\n",
    "        if current_entity:\n",
    "            entities.append({'text': current_entity.strip(), 'type': current_type})\n",
    "        return entities\n",
    "'''\n",
    "\n",
    "with open(f'{MODEL_PATH}/inference.py', 'w') as f:\n",
    "    f.write(inference_code)\n",
    "\n",
    "print(f\"Inference code saved to {MODEL_PATH}/inference.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook trains a **joint DeBERTa-v3 model** that performs three tasks simultaneously:\n",
    "\n",
    "1. **Multi-label Tool Classification** - Predicts which tools (character_data, rulebook, session_notes) are needed\n",
    "2. **Intent Classification** - Predicts the specific intent for each selected tool (61 total intents)\n",
    "3. **Named Entity Recognition** - Extracts D&D entities using BIO tagging with CRF\n",
    "\n",
    "### Key Features:\n",
    "- **Shared encoder** learns representations beneficial for all three tasks\n",
    "- **CRF layer** for NER ensures valid BIO tag sequences\n",
    "- **Layer-wise learning rate decay** - higher LR for classification heads\n",
    "- **Early stopping** based on combined F1 score\n",
    "- **Google Drive persistence** for Colab\n",
    "\n",
    "### Model saved to:\n",
    "- `{DRIVE_PATH}/models/joint_deberta/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
